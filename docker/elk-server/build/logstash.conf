######### Input #########
input {
  file {
    path => "/usr/local/tomcat/logs/dcat-harvester.log"
    # having start_position set to beginning will ensure we parse the entire log file - useful for debugging
    # start_position => "beginning"
    # setting sincedb_path to /dev/null will ensure we always parse logs fresh every time - useful for debugging
    # sincedb_path => "/dev/null"
  }
  file {
    path => "/usr/local/tomcat/logs/dcat-admin.log"
    # having start_position set to beginning will ensure we parse the entire log file - useful for debugging
    # start_position => "beginning"
    # setting sincedb_path to /dev/null will ensure we always parse logs fresh every time - useful for debugging
    # sincedb_path => "/dev/null"
  }
}
######### Input #########

######### Filter #########
filter {
  # Extract event severity, timestamp and logevent type
  grok {
    match => { "message" => "%{DATESTAMP:timestamp} %{WORD:severity}.+?([a-zA-Z]+\.)+.+?- \[%{WORD:logger}\] \[%{WORD:result}\] %{GREEDYDATA:rawContent}" }
      tag_on_failure => [ "fail" ]
  }
  grok {
    match => { "rawContent" => "%{DATA:event}: %{GREEDYDATA:content}" }
    tag_on_failure => [ "fail" ]
  }
  kv {
    source => "content"
    field_split => ", "
    value_split => "="
  }
  mutate {
    remove_field => [ "content, rawContent, logger" ]
  }
}
######### Filter #########

######### Output #########
output {
  if "fail" in [tags] {
  } else {
    # Send to console - useful for testing
    # stdout {
   #codec => rubydebug
     #}

    # Send directly to local Elasticsearch
    elasticsearch {
      ### TODO: Finalise template ###
      #template => "/etc/logstash/conf.d/difi_template.json"
      index => "difi-%{+YYYY.MM.dd}"
      #template_overwrite => true
    }
  }
}
######### Output #########
